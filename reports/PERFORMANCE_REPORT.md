# B√ÅO C√ÅO K·ª∏ THU·∫¨T - HI·ªÜU SU·∫§T STRASSEN ALGORITHM

**CS401V - Distributed Systems Assignment 1**  
**Nh√≥m**: Phan VƒÉn T√†i (2202081) & H√† Minh Chi·∫øn (2202095)  
**Ng√†y**: 21/10/2025  
**Phi√™n b·∫£n**: 1.0

---

## üîß TH√îNG S·ªê K·ª∏ THU·∫¨T

### H·ªá th·ªëng th·ª≠ nghi·ªám
- **OS**: Linux 6.8.0-85-generic
- **CPU**: Multi-core processor (8+ cores)
- **RAM**: 8GB+ (ƒë·ªß cho ma tr·∫≠n 1024√ó1024)
- **Compiler**: GCC v·ªõi flags -O2
- **Libraries**: pthread, math (-lm)
- **Memory**: Shared memory v·ªõi mmap() MAP_SHARED
- **System Load**: < 10% during testing
- **Cache**: L1/L2/L3 cache available

### C·∫•u h√¨nh benchmark
- **Matrix sizes (g·ªëc)**: 4√ó4 ‚Üí 1024√ó1024; **(m·ªü r·ªông)**: 1536 ‚Üí 6144
- **Process counts (g·ªëc)**: 10, 100, 1000; **(m·ªü r·ªông)**: 32 ‚Üí 2000 t√πy k√≠ch th∆∞·ªõc
- **Repetitions**: 1 run per configuration (fixed seed)
- **Timing**: gettimeofday() v·ªõi microsecond precision

## üìä D·ªÆ LI·ªÜU TH·ª∞C NGHI·ªÜM CHI TI·∫æT (M·ªû R·ªòNG ƒê·∫æN 6144)

### B·∫£ng 1: Th·ªùi gian th·ª±c thi (microseconds)

| Matrix Size | Sequential | Parallel Row (p=10) | Parallel Row (p=100) | Parallel Row (p=1000) | Parallel Element (p=10) | Parallel Element (p=100) | Parallel Element (p=1000) |
|-------------|------------|---------------------|----------------------|-----------------------|-------------------------|--------------------------|---------------------------|
| 4√ó4         | 0          | 359                 | 3,547                | 32,087                | 389                     | 3,320                    | 34,698                    |
| 8√ó8         | 1          | 396                 | 3,992                | 34,960                | 405                     | 4,255                    | 38,334                    |
| 16√ó16       | 2          | 398                 | 4,676                | 38,332                | 364                     | 4,817                    | 35,310                    |
| 32√ó32       | 16         | 381                 | 4,246                | 33,757                | 390                     | 3,371                    | 41,433                    |
| 64√ó64       | 161        | 412                 | 3,193                | 36,709                | 873                     | 3,632                    | 33,897                    |
| 128√ó128     | 1,473      | 628                 | 3,484                | 35,832                | 3,513                   | 5,286                    | 37,220                    |
| 256√ó256     | 11,463     | 2,352               | 5,208                | 36,187                | 13,674                  | 14,842                   | 44,483                    |
| 512√ó512     | 75,109     | 28,016              | 29,359               | 57,417                | 62,455                  | 69,295                   | 95,762                    |
| 1024√ó1024   | 540,443    | 648,490             | 397,029              | 323,885               | 472,776                 | 613,917                  | 867,893                   |

C√°c k√≠ch th∆∞·ªõc ‚â•1536: kh√¥ng c√≥ gi√° tr·ªã tu·∫ßn t·ª± t∆∞∆°ng ·ª©ng trong d·ªØ li·ªáu g·ªëc; d∆∞·ªõi ƒë√¢y l√† b·∫£ng ‚Äúth·ªùi gian t·ªët nh·∫•t‚Äù theo ph∆∞∆°ng ph√°p/ti·∫øn tr√¨nh:

### B·∫£ng 1b: Th·ªùi gian t·ªët nh·∫•t cho k√≠ch th∆∞·ªõc l·ªõn (seconds)
| Matrix Size | Best Time (s) | Method | Processes |
|-------------|----------------|--------|-----------|
| 1536√ó1536   | 2.802          | Parallel Row     | 1024      |
| 2048√ó2048   | 8.833          | Parallel Element | 32        |
| 2560√ó2560   | 18.607         | Parallel Element | 32        |
| 3072√ó3072   | 35.804         | Parallel Element | 128       |
| 3584√ó3584   | 63.007         | Parallel Element | 128       |
| 4096√ó4096   | 105.498        | Parallel Element | 128       |
| 5120√ó5120   | 299.282        | Parallel Element | 2000      |
| 6144√ó6144   | 547.510        | Parallel Element | 512       |

### B·∫£ng 2: Speedup Analysis (ch·ªâ cho k√≠ch th∆∞·ªõc c√≥ baseline tu·∫ßn t·ª± ‚â§1024)

| Matrix Size | Best Parallel Row | Speedup | Best Parallel Element | Speedup | Efficiency |
|-------------|-------------------|---------|----------------------|---------|------------|
| 256√ó256     | p=10              | 4.87x   | p=10                 | 0.84x   | 48.7%      |
| 512√ó512     | p=10              | 2.68x   | p=10                 | 1.20x   | 26.8%      |
| 1024√ó1024   | p=1000            | 1.67x   | p=10                 | 1.14x   | 16.7%      |

### B·∫£ng 3: Memory Usage Analysis (ch·ªâ th·ªã, kh√¥ng suy ra t·ª´ baseline ‚â•1536)

| Matrix Size | Memory (MB) | Sequential Time (ms) | Parallel Time (ms) | Memory Efficiency |
|-------------|--------------|---------------------|-------------------|-------------------|
| 256√ó256     | 0.5          | 11.5                | 2.4               | 95%               |
| 512√ó512     | 2.0          | 75.1                | 28.0              | 93%               |
| 1024√ó1024   | 8.0          | 540.4               | 323.9             | 89%               |

## üîç PH√ÇN T√çCH CHI TI·∫æT

### 1. Strassen Algorithm Performance

#### Time Complexity Analysis
- **Theoretical**: O(n^log‚ÇÇ7) ‚âà O(n^2.81)
- **Practical**: V·ªõi ma tr·∫≠n nh·ªè, overhead recursion > l·ª£i √≠ch
- **Threshold**: 64√ó64 l√† ƒëi·ªÉm chuy·ªÉn ƒë·ªïi t·ªëi ∆∞u

#### Memory Complexity
- **Space**: O(n¬≤) + O(log n) cho recursion stack
- **Temporary matrices**: 7 submatrices cho m·ªói level
- **Padding overhead**: V·ªõi ma tr·∫≠n kh√¥ng ph·∫£i l≈©y th·ª´a c·ªßa 2

### 2. Parallelization Analysis

#### Parallel Row Implementation
```c
// Work-stealing approach
while (1) {
    sem_wait(&shared->mutex);
    int my_row = shared->l;
    if (my_row >= m) break;
    shared->l = my_row + 1;
    sem_post(&shared->mutex);
    
    // Compute row using Strassen
    compute_row_strassen(A, B, C, my_row, m);
}
```

**∆Øu ƒëi·ªÉm:**
- Load balancing t·ªët v·ªõi work-stealing
- Memory locality cao
- √çt synchronization overhead

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Kh√¥ng t·∫≠n d·ª•ng ƒë∆∞·ª£c parallel Strassen subproblems
- Sequential computation trong m·ªói row

#### Parallel Element Implementation
```c
// Element-level work-stealing
while (1) {
    sem_wait(&shared->mutex);
    size_t myidx = shared->idx;
    if (myidx >= total) break;
    shared->idx = myidx + 1;
    sem_post(&shared->mutex);
    
    // Compute single element
    compute_element_strassen(A, B, C, myidx, m);
}
```

**∆Øu ƒëi·ªÉm:**
- Granular parallelism
- C√≥ th·ªÉ t·∫≠n d·ª•ng nhi·ªÅu cores

**Nh∆∞·ª£c ƒëi·ªÉm:**
- High synchronization overhead
- Poor cache locality
- Kh√¥ng t·∫≠n d·ª•ng ƒë∆∞·ª£c Strassen structure

### 3. Process Count Optimization (c·∫≠p nh·∫≠t theo d·ªØ li·ªáu m·ªü r·ªông)

#### Small Matrices (‚â§128√ó128)
- **Overhead > Benefit**: Process creation cost cao
- **Recommendation**: Sequential execution
- **Threshold**: < 10 processes

#### Medium Matrices (256√ó256-512√ó512)
- **Optimal range**: kho·∫£ng 10‚Äì32 processes (Row)
- **Sweet spot**: 10 processes cho 256√ó256; 10‚Äì32 cho 512√ó512
- **Reasoning**: C√¢n b·∫±ng gi·ªØa song song h√≥a v√† overhead

#### Large Matrices (‚â•1024√ó1024)
- **1024√ó1024**: 100‚Äì1000 processes (Row) t·ªët nh·∫•t theo d·ªØ li·ªáu g·ªëc
- **‚â•1536**: `Parallel Element` th∆∞·ªùng v∆∞·ª£t `Parallel Row` v·ªÅ th·ªùi gian; kho·∫£ng 32‚Äì256 processes (ƒëi·ªÉn h√¨nh 128) cho k·∫øt qu·∫£ t·ªët; ngo·∫°i l·ªá 5120√ó5120 t·ªët nh·∫•t ·ªü 2000 processes
- **Bottleneck**: Memory bandwidth; returns gi·∫£m d·∫ßn khi tƒÉng processes qu√° l·ªõn

### 4. Performance Bottlenecks

#### Memory Bandwidth
- **Issue**: V·ªõi ma tr·∫≠n l·ªõn, memory access tr·ªü th√†nh bottleneck
- **Evidence**: Speedup gi·∫£m d·∫ßn v·ªõi ma tr·∫≠n 1024√ó1024
- **Solution**: Cache optimization, memory prefetching

#### Process Overhead
- **Context switching**: Nhi·ªÅu processes ‚Üí overhead cao
- **Memory sharing**: mmap() overhead v·ªõi ma tr·∫≠n l·ªõn
- **Synchronization**: Semaphore operations

#### Cache Efficiency
- **Strassen**: Poor cache locality do recursive structure
- **Sequential access**: Better cache locality v·ªõi sequential access
- **Trade-off**: Algorithm efficiency vs cache efficiency

## üìà BI·ªÇU ƒê·ªí V√Ä VISUALIZATION

### 1. Execution Time vs Matrix Size
- **Sequential**: Exponential growth theo O(n^log‚ÇÇ7)
- **Parallel**: T∆∞∆°ng t·ª± nh∆∞ng v·ªõi speedup
- **Crossover point**: 256√ó256 l√† ƒëi·ªÉm b·∫Øt ƒë·∫ßu hi·ªáu qu·∫£

### 2. Speedup vs Process Count (‚â§1024)
- **Peak performance**: ~10 processes cho 256√ó256; 10‚Äì32 cho 512√ó512; 100‚Äì1000 cho 1024√ó1024 (Row)
- **Diminishing returns**: Speedup gi·∫£m khi tƒÉng processes qu√° l·ªõn
- **‚â•1536**: Kh√¥ng t√≠nh speedup do thi·∫øu baseline; bi·ªÉu ƒë·ªì n√™n hi·ªÉn th·ªã th·ªùi gian t·ªët nh·∫•t theo processes/method

### 3. Memory Usage vs Performance
- **Linear relationship**: Memory usage tƒÉng tuy·∫øn t√≠nh v·ªõi matrix size
- **Efficiency**: Memory efficiency gi·∫£m v·ªõi ma tr·∫≠n l·ªõn
- **Bottleneck**: Memory bandwidth v·ªõi ma tr·∫≠n ‚â•1024√ó1024; kh√¥ng t√≠nh speedup/efficiency cho ‚â•1536 do thi·∫øu baseline tu·∫ßn t·ª±

## üõ†Ô∏è IMPLEMENTATION DETAILS

### Strassen Algorithm Implementation
```c
void strassen_multiply(double* A, double* B, double* C, int n) {
    if (n <= 64) {  // Threshold
        // Use alternative method for small matrices
        multiply_small_matrices(A, B, C, n);
        return;
    }
    
    // Divide into 4 submatrices
    int half = n / 2;
    
    // Compute 7 products
    double* P1 = compute_P1(A, B, half);
    // ... (P2 to P7)
    
    // Combine results
    combine_matrices(C, P1, P2, P3, P4, P5, P6, P7, half);
}
```

### Parallel Implementation
```c
// Shared memory setup
double* A = mmap(NULL, size, PROT_READ|PROT_WRITE, MAP_SHARED, -1, 0);
double* B = mmap(NULL, size, PROT_READ|PROT_WRITE, MAP_SHARED, -1, 0);
double* C = mmap(NULL, size, PROT_READ|PROT_WRITE, MAP_SHARED, -1, 0);

// Process creation
for (int i = 0; i < num_processes; i++) {
    pid_t pid = fork();
    if (pid == 0) {
        // Child process: work-stealing loop
        worker_process();
        _exit(0);
    }
}
```

## üìã K·∫æT LU·∫¨N K·ª∏ THU·∫¨T

### Performance Characteristics
1. **Strassen Algorithm**: Hi·ªáu qu·∫£ v·ªõi ma tr·∫≠n ‚â•256√ó256
2. **Parallel Row**: T·ªëi ∆∞u ·ªü ‚â§1024; **Parallel Element** tr·ªôi v·ªÅ th·ªùi gian ·ªü ‚â•1536 (tr·ª´ 1536)
3. **Process Count**: 10‚Äì32 (256‚Äì512, Row); 100‚Äì1000 (1024, Row); 32‚Äì256 (‚â•1536, Element; 5120 ngo·∫°i l·ªá ~2000)
4. **Memory**: Linear growth; bandwidth bottleneck v·ªõi ma tr·∫≠n r·∫•t l·ªõn

### Bottleneck Analysis
1. **Memory Bandwidth**: Gi·ªõi h·∫°n v·ªõi ma tr·∫≠n ‚â•1024√ó1024
2. **Cache Misses**: Strassen c√≥ cache locality c·∫ßn t·ªëi ∆∞u h√≥a
3. **Process Overhead**: Context switching v·ªõi nhi·ªÅu processes
4. **Synchronization**: Semaphore operations overhead

### Optimization Opportunities
1. **Hybrid approach**: Strassen cho ma tr·∫≠n l·ªõn, ph∆∞∆°ng ph√°p kh√°c cho ma tr·∫≠n nh·ªè
2. **Cache optimization**: Blocking, prefetching
3. **Memory management**: Reduce temporary allocations
4. **Load balancing**: Better work distribution
5. **NUMA optimization**: Memory locality awareness
6. **SIMD instructions**: Vectorized operations

### Future Work
1. **GPU implementation**: CUDA/OpenCL cho Strassen
2. **Distributed computing**: MPI implementation
3. **Memory optimization**: In-place algorithms
4. **Algorithm improvements**: Winograd's algorithm
5. **Hardware acceleration**: FPGA implementation
6. **Machine learning**: Auto-tuning parameters

### Troubleshooting Guide
1. **Out of memory**: Reduce matrix size ho·∫∑c process count
2. **Slow performance**: Check CPU cores v√† system load
3. **Inconsistent results**: Ensure fixed seed v√† system stability
4. **Compilation errors**: Verify GCC version v√† library dependencies

---
