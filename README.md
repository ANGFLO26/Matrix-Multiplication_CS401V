# Matrix Multiplication Parallel Computing

**CS401V - Distributed Systems Assignment 1**  
**Group Members:** Phan VÄƒn TÃ i (2202081) & HÃ  Minh Chiáº¿n (2202095)

## ğŸ“‹ Project Overview

This project implements and compares three different approaches to matrix multiplication:

1. **Sequential**: Traditional O(nÂ³) algorithm using single process
2. **Parallel Row**: Row-level parallelization with work-stealing using multiple processes
3. **Parallel Element**: Element-level parallelization with work-stealing using multiple processes

## ğŸ¯ Objectives

- Implement matrix multiplication using process-based parallel computing
- Compare performance of sequential vs parallel approaches
- Analyze scalability with different matrix sizes and process counts
- Demonstrate understanding of parallel programming concepts (fork, mmap, semaphores)

## ğŸš€ Quick Start

### Prerequisites
- Linux system with GCC compiler
- pthread library support
- Multi-core CPU (recommended 8+ cores)

### Installation & Compilation
```bash
# Clone the repository
git clone <repository-url>
cd assignment1_code

# Compile all programs
make all

# Or compile individually
gcc src/sequentialMult.c -o compiled/sequentialMult
gcc src/parallelRowMult.c -o compiled/parallelRowMult -pthread
gcc src/parallelElementMult.c -o compiled/parallelElementMult -pthread
```

### Running Tests
```bash
# Quick performance test
./scripts/quick_test.sh

# Full benchmark with all configurations
./scripts/benchmark_report.sh
```

## ğŸ“Š Performance Results

### Key Findings
- **Best Performance**: Parallel Row with 1000 processes
- **Maximum Speedup**: 7.1x for 1000Ã—1000 matrices
- **Memory Bottleneck**: Observed with 2000Ã—2000 matrices

### Performance Summary
| Matrix Size | Sequential | Parallel Row (1000p) | Speedup |
|-------------|------------|---------------------|---------|
| 100Ã—100     | 2.2ms      | 0.7ms               | 3.2x    |
| 1000Ã—1000   | 3.47s      | 0.49s               | 7.1x    |
| 2000Ã—2000   | 32.23s     | 5.69s               | 5.7x    |

## ğŸ—ï¸ Project Structure

```
assignment1_code/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ sequentialMult.c    # Sequential implementation
â”‚   â”œâ”€â”€ parallelRowMult.c   # Row-level parallel implementation
â”‚   â”œâ”€â”€ parallelElementMult.c # Element-level parallel implementation
â”‚   â””â”€â”€ common.h           # Common utilities
â”œâ”€â”€ compiled/               # Compiled executables
â”œâ”€â”€ scripts/                # Utility scripts
â”œâ”€â”€ reports/                # Performance analysis reports
â”œâ”€â”€ docs/                   # Assignment documentation
â”œâ”€â”€ requirements.txt        # System requirements
â””â”€â”€ README.md              # This file
```

## ğŸ”§ Technical Implementation

### Key Technologies
- **Process Management**: `fork()`, `wait()`, `_exit()`
- **Memory Sharing**: `mmap()` with `MAP_SHARED`
- **Synchronization**: Semaphores for shared variables
- **Timing**: `gettimeofday()` for microsecond precision

### Parallelization Strategy
- **Row-level**: Each process computes entire rows
- **Element-level**: Each process computes individual elements
- **Work-stealing**: Dynamic load balancing using shared indices

## ğŸ“ˆ Performance Analysis

### Scaling Behavior
- **Small matrices** (< 100Ã—100): Sequential is fastest due to parallel overhead
- **Medium matrices** (100Ã—1000): Parallel Row with 10-100 processes optimal
- **Large matrices** (1000Ã—1000+): Parallel Row with 1000 processes optimal
- **Very large matrices** (2000Ã—2000+): Memory bandwidth becomes bottleneck

### Key Insights
1. **Parallel overhead** significant for small matrices
2. **Row-level parallelization** more efficient than element-level
3. **Process count optimization** depends on matrix size
4. **Memory bandwidth** limits scaling for very large matrices

## ğŸ“š Reports & Documentation

- **FINAL_REPORT.md**: Comprehensive performance analysis
- **PERFORMANCE_REPORT.md**: Technical implementation details
- **SUMMARY_RESULTS.md**: Quick reference for key results

## ğŸ› ï¸ Usage Examples

### Basic Usage
```bash
# Sequential matrix multiplication
./compiled/sequentialMult 1000

# Parallel row multiplication
./compiled/parallelRowMult 1000 10

# Parallel element multiplication
./compiled/parallelElementMult 1000 10
```

### Advanced Benchmarking
```bash
# Test all configurations
./scripts/benchmark_report.sh

# Custom matrix sizes
./compiled/sequentialMult 2000
./compiled/parallelRowMult 2000 1000
```

## ğŸ› Troubleshooting

### Common Issues
- **Compilation errors**: Ensure GCC and pthread are installed
- **Out of memory**: Reduce matrix size or process count
- **Slow performance**: Check CPU core count and system load

### Performance Tips
- Use 10-100 processes for most cases
- Ensure sufficient RAM for large matrices
- Close unnecessary applications during benchmarking

## ğŸ“‹ Requirements

See `requirements.txt` for detailed system requirements and installation instructions.

## ğŸ‘¥ Team Members

- **Phan VÄƒn TÃ i** (2202081) - Implementation & Testing
- **HÃ  Minh Chiáº¿n** (2202095) - Analysis & Documentation

## ğŸ“„ License

This project is part of CS401V - Distributed Systems coursework.

---

*For detailed performance analysis and technical implementation, see the reports in the `reports/` directory.*
# Matrix-Multiplication_CS401V
